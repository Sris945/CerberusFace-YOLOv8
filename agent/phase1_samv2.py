# -----------------------------------------------------------------------------
# Phase 1: AI Struct Agent - v1.0.2 (LangChain + IBM Watsonx.ai)
# -----------------------------------------------------------------------------
# This module implements the first phase of the AI Struct Agent, focusing on
# project discovery, pain point analysis, and initial strategy formulation.

import os
import sys
import json
import time
import shutil
import socket
import platform
import subprocess
import argparse
from pathlib import Path
import re

# LANGCHAIN CHANGE: Import LangChain components instead of the direct IBM library
from langchain_ibm import WatsonxLLM
from langchain_core.output_parsers import JsonOutputParser

# -----------------------------------------------------------------------------
# Configuration
# -----------------------------------------------------------------------------
AGENT_VERSION = "1.0.0" # Updated version for LangChain

# Configuration for IBM Watsonx.ai
# NOTE: It is recommended to use environment variables for security.
# Configuration for IBM Watsonx.ai - Get from environment variables
IBM_API_KEY = os.getenv("IBM_API_KEY", "jImzU56SoPzBgqpYcN6XcYmOnA1QQuB3ATeRk_81B7pm")
IBM_URL = os.getenv("IBM_URL", "https://us-south.ml.cloud.ibm.com")
IBM_PROJECT_ID = os.getenv("IBM_PROJECT_ID", "9229f6e3-2ad7-45eb-ab0e-333dc7723dfb")
MODEL_ID = "ibm/granite-3-3-8b-instruct"
if not all([IBM_API_KEY, IBM_URL, IBM_PROJECT_ID]):
    raise RuntimeError("Please set the IBM_API_KEY, IBM_URL, and IBM_PROJECT_ID.")

# --- Enhanced, Persona-Specific Markdown Templates ---
PERSONAS = ["Developer", "Data Scientist", "Researcher", "Student"]
# -----------------------------------------------------------------------------
# Report Templates
# -----------------------------------------------------------------------------
TEMPLATES = {
    "Developer": """# AI Refactoring Report for {project_root}

## Executive Summary
{executive_summary}

## Project Metadata
- **Environment Fingerprint:** {fingerprint}
- **Git Info:** {git_info}
- **Dependencies:** {dependencies}
- **Environment:** {environment}

## Pain Points
{pain_points}

## Proposed Use Cases
- {use_cases}

## Success Metrics
{success}

## Feasibility Notes
- {feasibility}

---

*Generated by AI Refactoring Agent v{version}*
""",

    "Data Scientist": """# AI Data Science Project Refactor Report

## Summary
{executive_summary}

## Project Metadata
- **Data Environment Fingerprint:** {fingerprint}
- **Git Info:** {git_info}
- **Dependencies:** {dependencies}
- **Environment:** {environment}

## Identified Pain Points
{pain_points}

## Recommended Use Cases
- {use_cases}

## Success Metrics
{success}

## Feasibility
- {feasibility}

---

*AI Agent v{version}*
""",

    "Researcher": """# Research Project AI Refactor Report

## Summary
{executive_summary}

## Metadata
- **Environment:** {environment}
- **Fingerprint:** {fingerprint}
- **Git Info:** {git_info}
- **Dependencies:** {dependencies}

## Pain Points
{pain_points}

## Research Use Cases
- {use_cases}

## Success Metrics
{success}

## Feasibility Notes
- {feasibility}

---
*Generated by AI Agent v{version}*
""",

    "Student": """# Student-Friendly Refactor Report

## Quick Summary
{executive_summary}

## Project Info
- **Environment:** {environment}
- **Dependencies:** {dependencies}
- **Git Info:** {git_info}
- **Fingerprint:** {fingerprint}

## Issues Found
{pain_points}

## Suggested Use Cases
- {use_cases}

## Success Metrics
{success}

## Feasibility
- {feasibility}

---
*AI Learning Agent v{version}*
"""
}

FALLBACK_TEMPLATE = TEMPLATES["Developer"] # Use Developer as a robust default

# -----------------------------------------------------------------------------
# Helpers
# -----------------------------------------------------------------------------
def check_internet(host="8.8.8.8", port=53, timeout=3):
    try:
        socket.setdefaulttimeout(timeout)
        socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect((host, port))
        return True
    except Exception:
        return False

def check_disk(threshold_gb=1):
    total, used, free = shutil.disk_usage(Path.cwd())
    free_gb = free / (1024 ** 3)
    return free_gb >= threshold_gb, round(free_gb, 2)

def get_project_fingerprint(root: Path):
    """Counts files by common extensions to create a project fingerprint."""
    extensions = {
        "Python": [".py"], "Notebook": [".ipynb"], "Data": [".csv", ".json", ".parquet", ".db", ".sqlite3"],
        "Config": [".yaml", ".yml", ".toml", ".ini", ".env"], "Web": [".html", ".css", ".js"],
        "Docs": [".md", ".rst"], "Container": ["Dockerfile"], "Other": []
    }
    fingerprint = {key: 0 for key in extensions}

    # Flatten the list of extensions for easier lookup
    ext_map = {ext: key for key, ext_list in extensions.items() for ext in ext_list}

    for p in root.rglob("*"):
        if p.is_file():
            if p.name in extensions["Container"]:
                fingerprint["Container"] += 1
                continue

            category = ext_map.get(p.suffix, "Other")
            fingerprint[category] += 1

    return {k: v for k, v in fingerprint.items() if v > 0} # Return only non-zero counts

def get_git_info(root: Path):
    """Gets Git status if the project is a git repository."""
    if not (root / ".git").exists():
        return {"status": "Not a Git repository."}
    try:
        status = subprocess.check_output(["git", "-C", str(root), "status", "--porcelain"], text=True).strip()
        remote = subprocess.check_output(["git", "-C", str(root), "remote", "-v"], text=True).strip()
        branch = subprocess.check_output(["git", "-C", str(root), "rev-parse", "--abbrev-ref", "HEAD"], text=True).strip()
        return {
            "status": "Clean" if not status else "Dirty (has uncommitted changes)",
            "branch": branch,
            "remote": remote.split('\n')[0] if remote else "No remote configured"
        }
    except Exception as e:
        return {"status": f"Could not retrieve Git info: {e}"}

def parse_existing_dependencies(root: Path):
    """Parses common dependency files to find declared dependencies."""
    if (root / "requirements.txt").exists():
        return (root / "requirements.txt").read_text().strip().split('\n')
    if (root / "pyproject.toml").exists():
        content = (root / "pyproject.toml").read_text()
        deps = re.findall(r'\[tool\.poetry\.dependencies\]\n(.*?)\n\[', content, re.DOTALL)
        if deps:
            return [line.split('=')[0].strip() for line in deps[0].strip().split('\n') if line]
    return ["No common dependency file found."]


def summarize_and_cluster_pain_points(pain_points):
    """Uses an LLM to thematically group and name pain points."""
    if len(pain_points) < 2:
        return {"General Issues": pain_points}

    prompt = (
        "You are an expert project manager. Analyze the following user-stated pain points. "
        "Group them into 2-3 meaningful, thematic clusters. "
        "Return a JSON object where keys are your descriptive cluster names and values are the lists of pain points belonging to that cluster.\n\n"
        f"Pain Points:\n- {'\n- '.join(pain_points)}\n\n"
        "Example Output: {\"Code Modularity Issues\": [\"point a\", \"point b\"], \"Data Accessibility\": [\"point c\"]}\n"
        "JSON output only."
    )
    try:
        # LANGCHAIN CHANGE: Use WatsonxLLM and a JsonOutputParser
        parameters = {
            "decoding_method": "greedy",
            "max_new_tokens": 2048,  # Increased from 1024 for more detailed output
            "min_new_tokens": 1,
            "temperature": 0.0,
        }

        llm = WatsonxLLM(
            model_id=MODEL_ID,
            url=IBM_URL,
            apikey=IBM_API_KEY,
            project_id=IBM_PROJECT_ID,
            params=parameters
        )

        # Using a JSON parser is more robust than manual string cleaning
        parser = JsonOutputParser()
        chain = llm | parser

        # The chain will invoke the LLM and automatically parse the JSON output
        return chain.invoke(prompt)

    except Exception as e:
        # Fallback to simple clustering if AI fails
        print(f"\n‚ö†Ô∏è AI clustering failed: {e}")
        return {"General Issues": pain_points}

# -----------------------------------------------------------------------------
# Main Flow
# -----------------------------------------------------------------------------
def execute_phase1(project_path: str, persona: str, pain_points: list, use_cases: list, success_metrics: str) -> bool:
    """
    Executes the core logic of Phase 1.
    This function is designed to be called from an orchestrator.
    It takes all inputs as parameters and does not use `input()` or `sys.exit()`.
    Returns True on success, False on failure.
    """
    project_root = Path(project_path)
    if not project_root.exists() or not project_root.is_dir():
        print(f"‚ùå Invalid project path: {project_root}")
        return False

    # --- Build Enhanced Metadata ---
    print("\nüîç Gathering project intelligence...")
    metadata = {
        "project_root": str(project_root.resolve()),
        "persona": persona,
        "pain_points": summarize_and_cluster_pain_points(pain_points),
        "use_cases": use_cases,
        "success_metrics": success_metrics,
        "project_fingerprint": get_project_fingerprint(project_root),
        "git_info": get_git_info(project_root),
        "dependencies": parse_existing_dependencies(project_root),
        "environment": {
            "python_version": platform.python_version(),
            "os": platform.system() + " " + platform.release(),
            "agent_version": AGENT_VERSION
        },
        "feasibility_notes": []
    }

    # --- Feasibility Checks ---
    if not check_internet():
        metadata["feasibility_notes"].append("No internet connection: AI calls may fail.")
    ok_disk, free_gb = check_disk()
    if not ok_disk:
        metadata["feasibility_notes"].append(f"Low disk space: {free_gb} GB available.")

    print(json.dumps(metadata, indent=2))

    # --- Generate via AI with Dynamic Prompt ---
    # --- Generate via AI with Dynamic Prompt ---
    print("\nüß† Asking AI Architect for a detailed strategic brief...")

    # NEW PROMPT: This prompt is much more detailed and asks for a structured output
    # using Markdown headings. This will guide the AI to produce a comprehensive plan.
    prompt = f"""
You are a Principal AI Architect and Project Strategist. Your task is to analyze the provided project data and generate a "Strategic Project Brief". This document will serve as the foundational plan for a complete project refactoring.

The output should be well-structured, using Markdown headings for each section as specified below.

**Discovery Data:**
- **Persona:** {metadata['persona']}
- **Stated Pain Points:** {json.dumps(metadata['pain_points'])}
- **Project File Fingerprint:** {json.dumps(metadata['project_fingerprint'])}
- **Current Git Status:** {json.dumps(metadata['git_info'])}
- **Desired Use Cases:** {', '.join(metadata['use_cases'])}

**Please generate the brief with the following structure:**

### 1. Executive Summary
- A high-level paragraph summarizing the project's current state, the primary goal of the refactoring, and the expected outcome.

### 2. Current State Analysis
- **User Profile & Challenges:** Briefly describe the user (a '{metadata['persona']}') and elaborate on how their stated pain points likely manifest in their daily workflow.
- **Technical Footprint:** Analyze the project's composition based on the file fingerprint. For example, a project heavy on `.py` and `.ipynb` files suggests a mix of application code and experimental analysis. Mention any key dependencies if relevant.
- **Development Status:** Interpret the Git status. A 'Dirty' status might indicate active, unmanaged development, while a 'Clean' status suggests a more stable baseline.

### 3. Proposed Strategic Direction
- Articulate a clear, high-level strategy for refactoring. For a '{metadata['persona']}', this might be "Transitioning from an experimental notebook-driven workflow to a modular, production-ready application structure with a clear separation of concerns."
- Explain how this new structure will directly address the user's primary pain points.

### 4. Key Success Metrics
- Reiterate the user-defined success metrics and frame them as the primary measure of the project's success.

### 5. Next Steps
- Conclude with a statement that sets the stage for the next phase: generating and applying the new, structured project layout.

**Generate only the text for the Strategic Project Brief, starting with the first heading.**
"""

    executive_summary = "AI generation failed. A structured project will be proposed based on the collected metadata."
    error_log = None
    try:
        # LANGCHAIN CHANGE: Use WatsonxLLM for structured text generation
        # INCREASED TOKEN LIMIT: We increase max_new_tokens to allow for a more detailed response.
        parameters = {
            "decoding_method": "greedy",
            "max_new_tokens": 4096, # Increased from 1024
            "min_new_tokens": 200,   # Increased minimum to encourage detail
            "temperature": 0.65, # Slightly reduced for more focused output
        }

        llm = WatsonxLLM(
            model_id=MODEL_ID,
            url=IBM_URL,
            apikey=IBM_API_KEY,
            project_id=IBM_PROJECT_ID,
            params=parameters
        )

        # For structured text output, just invoke the model directly
        executive_summary = llm.invoke(prompt).strip()

    except Exception as e:
        error_log = str(e)
        print(f"\n‚ö†Ô∏è AI summary generation failed: {e}")
    # --- Save Outputs ---
    tpl = TEMPLATES.get(persona, FALLBACK_TEMPLATE)

    pain_md = "\n".join(f"  - **{title}**: \n    - " + "\n    - ".join(items) for title, items in metadata["pain_points"].items())
    env_md = "\n".join(f"  - **{k}**: {v}" for k, v in metadata["environment"].items())
    fingerprint_md = "\n".join(f"  - **{k}**: {v} file(s)" for k, v in metadata["project_fingerprint"].items())
    git_md = "\n".join(f"  - **{k.capitalize()}**: {v}" for k, v in metadata["git_info"].items())
    deps_md = "- " + "\n- ".join(metadata["dependencies"])

    final_md = tpl.format(
        executive_summary=executive_summary,
        project_root=metadata["project_root"],
        fingerprint=fingerprint_md,
        pain_points=pain_md,
        use_cases="\n- ".join(metadata["use_cases"]),
        success=metadata["success_metrics"],
        git_info=git_md,
        environment=env_md,
        dependencies=deps_md,
        feasibility="\n- ".join(metadata["feasibility_notes"]) or "No issues found.",
        version=AGENT_VERSION
    )

    output_md_file = "phase1_discovery_document.md"
    output_json_file = "phase1_metadata.json"

    Path(output_md_file).write_text(final_md, encoding="utf-8")
    with open(output_json_file, "w", encoding="utf-8") as f:
        json.dump({"metadata": metadata, "discovery_doc": final_md}, f, indent=2)

    if error_log:
        Path("phase1_debug.log").write_text(error_log, encoding="utf-8")

    print("\n‚úÖ Phase 1 outputs written:")
    print(f" - {output_md_file} (Human-readable document)")
    print(f" - {output_json_file} (Machine-readable data for Phase 2)")
    if error_log:
        print(" - phase1_debug.log")

    return True # Signal success


def main():
    """
    Handles command-line execution and user input for standalone use.
    """
    parser = argparse.ArgumentParser(description="AI Struct Agent Phase 1 (LangChain + IBM Watsonx.ai)")
    parser.add_argument("--project-path", type=str, help="Path to unstructured project folder")
    parser.add_argument("--persona", type=str, choices=PERSONAS, help="User persona")
    args = parser.parse_args()

    # --- Collect Inputs ---
    project_path = args.project_path or input("üëâ Project path: ").strip()

    if args.persona:
        persona = args.persona
    else:
        persona = ""
        while persona not in PERSONAS:
            persona = input(f"üëâ Who are you? ({' / '.join(PERSONAS)}): ").strip()
            if persona not in PERSONAS:
                print(f"‚ùå Invalid persona. Please choose one of: {', '.join(PERSONAS)}")

    raw_pp = input("üëâ What problems do you face? (comma separated, e.g., 'messy imports, no tests'): ").split(",")
    pain_points = [p.strip() for p in raw_pp if p.strip()]

    raw_uc = input("üëâ What do you want this agent to do? (comma separated): ").split(",")
    use_cases = [u.strip() for u in raw_uc if u.strip()] or ["Suggest a clean project structure"]

    success_metrics = input("üëâ How will you measure success?: ").strip() or "A functional, easy-to-navigate project structure."

    # Call the core logic function with the collected inputs
    success = execute_phase1(project_path, persona, pain_points, use_cases, success_metrics)

    if not success:
        sys.exit(1)


if __name__ == "__main__":
    main()